\input texinfo
@settitle MLP_v1.0

@copying
Made by Morfinov and Changsky on October 2023
@end copying

@titlepage
@title MLP_v1.0 Guide
@subtitle «MLP»
@author Morfinov and Changsky
@vskip 
@insertcopying
@end titlepage

@ifnottex
@node Top
@top MLP_v1.0
@insertcopying
@end ifnottex

@menu
* General information:: This section provides general information about MLP.
* Part-1 Implementation:: This section contains information about the Implementation of a multilayer perceptron.
* Part-2 Bonus:: This section contains information about the Research.


@detailmenu
------------------------------------------------------------------------------------------------------------------------------------------------ 
 
 
-- FOR THOSE WHO READ CAREFULLY! --


In this description, we have posted only a brief summary of the product, made according to the assignment received.

If you have any additional questions or suggestions, we will be happy to consider them and will definitely think about it.

You can send all suggestions and comments to our email address or contact us in the rocket chat through our nicknames.

With respect for your time and attention.


...


------------------------------------------------------------------------------------------------------------------------------------------------ 

@end detailmenu

@end menu


@node General information
@chapter General information


In this project we has implement a simple artificial neural network 
in the form of a perceptron, which can be trained on an open dataset 
and perform recognition of 26 handwritten letters of the Latin alphabet.


@node Part-1 Implementation
@chapter Part-1 Implementation

@cindex implementation, part-1
The interface of the program provide the ability to:
@cindex index entry, another


@enumerate
@item
Can run the experiment on the test sample or on a part of it, given by a floating point number between 0 and 1 (where 0 is the empty sample - the degenerate situation, and 1 is the whole test sample). After the experiment, there will be an average accuracy, precision, recall, f-measure and total time spent on the experiment displayed on the screen;
@item
Can be load BMP images (image size can be up to 512x512) with Latin letters and classify them;
@item
Can be draw two-color square images by hand in a separate window;
@item
Can be start the real-time training process for a user-defined number of epochs with displaying the error control values for each training epoch. Will make a report as a graph of the error change calculated on the test sample for each training epoch;
@item
Can be run the training process using cross-validation for a given number of groups k;
@item
It is also possible to switch perceptron implementation (matrix or graph)
@item
It is also possible to switch the number of perceptron hidden layers (from 2 to 5)
@item
It is also possible to save to a file and load weights of perceptron from a file
@end enumerate



@node Part-2 Bonus
@chapter Part-2 Bonus

@cindex part-2, bonus
Research:
@cindex index entry, another

Based on the developed program, were compared the performance of two perceptron implementations: graph and matrix. To do this, we have done next steps:

@enumerate
@item
Trained the neural network and saved the obtained weights;
@item
Loaded the same weights for matrix and graph models and performed the experiment on the test sample 10 times, 100 times and 1000 times;
@item
Were averaged and recorded the data on time spent on graph and matrix models in a table in a separate markdown file at the root of the project; 
@end enumerate


@bye
